---
title: "Understanding Transformers in AI"
date: "2025-02-22"
image: "/images/transformers.jpg"
description: "A deep dive into the Transformer architecture, covering self-attention, positional encoding, and its impact on NLP."
tags: ["AI", "Transformers", "Deep Learning", "Mathematics"]
author: "Angelo Rosu"
---

## üìå Introduction
The **Transformer architecture**, introduced in *Attention Is All You Need* (Vaswani et al., 2017), revolutionized deep learning by replacing recurrent layers with **self-attention** mechanisms.  
This article explains **how Transformers work mathematically** and includes a **brief code implementation**.

---

## üîπ Key Concepts in Transformers
Transformers are built on three main ideas:
1. **Self-Attention**: Each word attends to all other words in a sentence.
2. **Positional Encoding**: Adds order information since Transformers lack recurrence.
3. **Multi-Head Attention**: Captures different relationships between words.

---

## üìä The Mathematics Behind Transformers
### **1Ô∏è‚É£ Self-Attention Mechanism**
Self-attention determines **how important each word is** to others in a sentence.
Mathematically, it is computed as:

$$
\text{Attention}(Q, K, V) = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right) V
$$

Where:
- $ Q $ (Query), $ K $ (Key), $ V $ (Value) are matrices derived from input embeddings.
- $ d_k $ is the scaling factor to prevent large values.
- **Softmax** normalizes scores.

### **2Ô∏è‚É£ Positional Encoding**
Since Transformers **don't have recurrence**, they require **positional encoding** to capture word order.

$$
PE_{(pos, 2i)} = \sin \left( \frac{pos}{10000^{2i/d_{model}}} \right)
$$

$$
PE_{(pos, 2i+1)} = \cos \left( \frac{pos}{10000^{2i/d_{model}}} \right)
$$

This creates a **unique position embedding** for each word.

---

## üñ•Ô∏è Implementing Transformers in Python
Here‚Äôs a **basic implementation** of **self-attention** using NumPy.

```python
import numpy as np

def scaled_dot_product_attention(Q, K, V):
    """Computes self-attention scores"""
    d_k = Q.shape[-1]
    scores = np.dot(Q, K.T) / np.sqrt(d_k)
    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)
    return np.dot(attention_weights, V)

# Example usage
Q = np.array([[1, 0, 0], [0, 1, 0]])  # Query vectors
K = np.array([[1, 0, 0], [0, 1, 0]])  # Key vectors
V = np.array([[0.5, 0.1, 0.3], [0.2, 0.7, 0.8]])  # Value vectors

output = scaled_dot_product_attention(Q, K, V)
print("Attention Output:\n", output)
